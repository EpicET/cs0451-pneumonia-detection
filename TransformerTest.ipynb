{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09dde81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c95536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Download chest x-ray (will take a minute or two)\n",
    "data_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce483aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path + \"/chest_xray/train\"\n",
    "test_path = data_path + \"/chest_xray/test\"\n",
    "val_path = data_path + \"/chest_xray/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a139cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/ml-0451/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/ml-0451/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(), #compression 3 rgb matrix into one for runtime\n",
    "    transforms.Resize((224, 224)), # can downsample here for runtime\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#Loading train, test, and validation datasets.\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_path, transform = transform)\n",
    "\n",
    "#dataloaders w 32 images each for batches (randomized)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_dataset.classes) # classes for the data (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28ba11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from ContrastiveVAE import ContrastiveVAE\n",
    "\n",
    "# contrastive loss function for VAE\n",
    "def supervised_contrastive_loss(embeddings, labels: torch.Tensor, temperature=0.1):\n",
    "    device = embeddings.device\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(device)\n",
    "\n",
    "    dot_product = (embeddings @ embeddings.T) / temperature\n",
    "    logits_max, _ = torch.max(dot_product, dim=1, keepdim=True)\n",
    "    logits = dot_product - logits_max.detach()\n",
    "\n",
    "    exp_logits = torch.exp(logits) * (1 - torch.eye(len(labels), device=device))\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)\n",
    "\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "    loss = -mean_log_prob_pos.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ae0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model: ContrastiveVAE, dataloader, optimizer, device, epochs=10, beta=1.0, contrastive_weight=1.0):\n",
    "    model.train()\n",
    "    recon_loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            recon_loss = recon_loss_fn(x_recon, x) / x.size(0)\n",
    "\n",
    "            kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            contrastive = supervised_contrastive_loss(mu, y)\n",
    "\n",
    "            loss = recon_loss + beta * kl_loss + contrastive_weight * contrastive\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c5a01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4465e9343440a689009f97d37e76c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2100.6436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb87838d23a494fa2a3f56752840a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 492.1164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7102d6fe9c20447cba1628b73241a708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 348.6070\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ContrastiveVAE(latent_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_vae(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=3,\n",
    "    beta=1.0,\n",
    "    contrastive_weight=5 # 5 ~208, 10 ~ 229\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99471b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent_vectors(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_mu = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            mu, _ = model.encode(x)\n",
    "            all_mu.append(mu.cpu().numpy()) #add all into np array\n",
    "            all_labels.append(y.numpy())\n",
    "\n",
    "    X = np.concatenate(all_mu, axis=0) # concatenate all latent vectors\n",
    "    y = np.concatenate(all_labels, axis=0) # concatenate all labels\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d2ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = extract_latent_vectors(model, train_loader, device)\n",
    "X_test, y_test = extract_latent_vectors(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b141e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 64) (5216,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdf4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer import Transformer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "latent_vectors = X_train.shape[0] # 5216 latent vectors\n",
    "latent_dim = X_train.shape[1] # 64 dimensions of latent space\n",
    "output_dim = 2\n",
    "\n",
    "# Initialize the Transformer model\n",
    "T = Transformer(input_dim = latent_dim, num_patches = 1, output_dim = output_dim, hidden_dim=latent_dim).to(device)\n",
    "\n",
    "# Train the Transformer model\n",
    "def train_transformer(model: Transformer, X_train, y_train, device, epochs=3, batch_size=32):\n",
    "    X = torch.tensor(X_train, dtype=torch.float32).to(device)  # [vectors, dim]\n",
    "    y = torch.tensor(y_train, dtype=torch.long).to(device)  # [labels]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        i = 0\n",
    "        while i < X.shape[0]: # this simulates batch loading method without dataloader\n",
    "            # Create batches\n",
    "            x_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)  \n",
    "            loss = criterion(outputs, y_batch)  \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            i += batch_size  \n",
    "\n",
    "        print(f\"Epoch {epoch + 1} - Loss: {total_loss / (X.shape[0] // batch_size):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4348310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.4442\n",
      "Epoch 2 - Loss: 0.1556\n",
      "Epoch 3 - Loss: 0.1438\n"
     ]
    }
   ],
   "source": [
    "train_transformer(T, X_train, y_train, device, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize a perceptron \n",
    "# p = Perceptron()\n",
    "# opt = PerceptronOptimizer(p)\n",
    "# p.loss(X, y)\n",
    "\n",
    "# loss = 1\n",
    "# score_vec = [] \n",
    "\n",
    "# while loss > 0 and len(score_vec) <= 1000:\n",
    "\n",
    "#     # save the old value of w for plotting later\n",
    "#     old_w = torch.clone(p.w)\n",
    "    \n",
    "#     # make an optimization step -- this is where the update actually happens\n",
    "#     # now p.w is the new value \n",
    "#     prev_length = len(score_vec)\n",
    "#     i = torch.randint(n, size = (1,))\n",
    "#     x_i = X[[i],:]\n",
    "#     y_i = y[i]\n",
    "#     local_loss = p.loss(x_i, y_i).item()\n",
    "#     score = p.score(X).mean()\n",
    "\n",
    "#     if local_loss > 0:\n",
    "#         opt.step(x_i, y_i)\n",
    "    \n",
    "#     if local_loss > 0:\n",
    "#         loss = p.loss(X, y).item()\n",
    "#         score = p.score(X).mean()\n",
    "#         score_vec.append(score)\n",
    "    \n",
    "#     if(len(score_vec) != prev_length):\n",
    "#         print(f\"Iteration {len(score_vec)}: Loss = {loss:.3f}, Score = {score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
